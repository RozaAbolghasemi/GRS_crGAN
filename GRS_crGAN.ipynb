{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow \n",
    "print(\"TensorFlow version:\", tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install keras==2.3.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    import keras\n",
    "    print(\"Keras is installed.\")\n",
    "    print(\"Keras version:\", keras.__version__)\n",
    "except ImportError:\n",
    "    print(\"Keras is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Input, Embedding, Dense, concatenate, Flatten, Subtract\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from utils2 import *\n",
    "from time import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker_layers = [128, 64, 32, 8]\n",
    "ranker_reg_layers = [0 for _ in range(len(ranker_layers))]\n",
    "discriminator_layers = [32, 8]\n",
    "discriminator_reg_layers = [0 for _ in range(len(discriminator_layers))]\n",
    "K = 2\n",
    "epochs = 20\n",
    "d_lr, r_lr = 0.01, 0.05\n",
    "\n",
    "out = True\n",
    "dataset = 'FoodData' #\"CarData\" #\n",
    "now = time()\n",
    "ranker_out_file = 'checkpoints/%d_%s_Ranker_%s.h5' % (now, dataset, ranker_layers)\n",
    "dis_out_file = 'checkpoints/%d_%s_Dis_%s.h5' % (now, dataset, discriminator_layers)\n",
    "log_file = 'checkpoints/%d_%s_%d.txt' % (now, dataset, K)\n",
    "\n",
    "pre_train = False\n",
    "in_time = 1549967959\n",
    "in_path = 'checkpoints'\n",
    "ranker_in_file = '%s/%s_%s_Ranker_%s.h5' % (in_path, in_time, dataset, ranker_layers)\n",
    "dis_in_file = '%s/%s_%s_Dis_%s.h5' % (in_path, in_time, dataset, discriminator_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pair-wise data from flie data/FoodData_train.dat...\n",
      "loading test data from file data/FoodData_test_ratings.lsvm...\n",
      "loading test data from file data/FoodData_all_ratings.lsvm...\n"
     ]
    }
   ],
   "source": [
    "num_users, num_items, train_u_input, train_i_input, train_j_input = get_pairwise_train_dataset(\n",
    "    path='data/%s_train.dat' % dataset)\n",
    "\n",
    "num_users += 1\n",
    "num_items += 1\n",
    "train_labels = [1 for _ in range(len(train_u_input))]\n",
    "\n",
    "\n",
    "testItems, testRatings = get_test_data(path='data/%s_test_ratings.lsvm' % dataset)\n",
    "AllItems, AllRatings = get_test_data(path='data/%s_all_ratings.lsvm' % dataset)\n",
    "\n",
    "\n",
    "input = Input(shape=(1,), dtype='float32', name='input')\n",
    "embedding_u = Embedding(input_dim=num_users, output_dim=int((discriminator_layers[0] - 1) / 3),\n",
    "                        name='rank_embedding_item',\n",
    "                        embeddings_initializer='random_normal', activity_regularizer=l2(discriminator_reg_layers[0]) , input_length=1)\n",
    "latent_u = Flatten()(embedding_u(input))\n",
    "embedding_model_u = Model(inputs=input, outputs=latent_u)\n",
    "train_u_latent = embedding_model_u.predict(train_u_input)\n",
    "\n",
    "embedding_i = Embedding(input_dim=num_items, output_dim=int((discriminator_layers[0] - 1) / 3),\n",
    "                        name='rank_embedding_item',\n",
    "                        embeddings_initializer='random_normal', activity_regularizer=l2(discriminator_reg_layers[0]), input_length=1)\n",
    "latent_i = Flatten()(embedding_i(input))\n",
    "embedding_model_i = Model(inputs=input, outputs=latent_i)\n",
    "train_i_latent = embedding_model_i.predict(train_i_input)\n",
    "train_j_latent = embedding_model_i.predict(train_j_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "d_u_input (InputLayer)          (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_i_input (InputLayer)          (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_j_input (InputLayer)          (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_r_input (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 31)           0           d_u_input[0][0]                  \n",
      "                                                                 d_i_input[0][0]                  \n",
      "                                                                 d_j_input[0][0]                  \n",
      "                                                                 d_r_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d_layer0 (Dense)                (None, 32)           1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "d_layer1 (Dense)                (None, 8)            264         d_layer0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "d_prediction (Dense)            (None, 1)            9           d_layer1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,297\n",
      "Trainable params: 1,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", name=\"d_layer0\", kernel_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=\"relu\", name=\"d_layer1\", kernel_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"d_prediction\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"d_...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=21, output_dim=10, name=\"dis_embedding_u\", input_length=1, embeddings_initializer=\"random_normal\", embeddings_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=7, output_dim=10, name=\"dis_embedding_i\", input_length=1, embeddings_initializer=\"random_normal\", embeddings_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=7, output_dim=10, name=\"dis_embedding_j\", input_length=1, embeddings_initializer=\"random_normal\", embeddings_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=21, output_dim=64, name=\"rank_embedding_user\", input_length=1, embeddings_initializer=\"random_normal\", embeddings_regularizer=<keras.reg...)`\n",
      "  if __name__ == '__main__':\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=7, output_dim=64, name=\"rank_embedding_item\", input_length=1, embeddings_initializer=\"random_normal\", embeddings_regularizer=<keras.reg...)`\n",
      "  if sys.path[0] == '':\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", name=\"r_layer1\", kernel_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", name=\"r_layer2\", kernel_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=\"relu\", name=\"r_layer3\", kernel_regularizer=<keras.reg...)`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"r_prediction\", kernel_initializer=\"lecun_uniform\")`\n",
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"r_...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "u_input (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "i_input (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "j_input (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dis_embedding_u (Embedding)     (None, 1, 10)        210         u_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dis_embedding_i (Embedding)     (None, 1, 10)        70          i_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dis_embedding_j (Embedding)     (None, 1, 10)        70          j_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 1)            12401       u_input[0][0]                    \n",
      "                                                                 i_input[0][0]                    \n",
      "                                                                 u_input[0][0]                    \n",
      "                                                                 j_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           dis_embedding_u[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 10)           0           dis_embedding_i[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 10)           0           dis_embedding_j[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 1)            0           model_4[1][0]                    \n",
      "                                                                 model_4[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 1)            1297        flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 subtract_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,048\n",
      "Trainable params: 12,751\n",
      "Non-trainable params: 1,297\n",
      "__________________________________________________________________________________________________\n",
      "init:  [0.95       0.9        0.91131472 0.9        0.70416667 0.59375   ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 1:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 2:  [0.95       0.9        0.91131472 0.9        0.70416667 0.59375   ]\n",
      "epoch 3:  [0.95       0.9        0.9        0.875      0.69166667 0.58125   ]\n",
      "epoch 4:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 5:  [0.95       0.9        0.9        0.875      0.69166667 0.58125   ]\n",
      "epoch 6:  [0.95       0.9        0.9        0.875      0.69166667 0.58125   ]\n",
      "epoch 7:  [0.95       0.9        0.9        0.875      0.69166667 0.58125   ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rozaabol/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8:  [0.95       0.9        0.9        0.875      0.69166667 0.58125   ]\n",
      "epoch 9:  [1.         0.925      0.91934264 0.8875     0.69583333 0.58541667] [best]\n",
      "epoch 10:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 11:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 12:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 13:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 14:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 15:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 16:  [0.95       0.9        0.9        0.875      0.69166667 0.58125   ]\n",
      "epoch 17:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 18:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n",
      "epoch 19:  [0.95       0.875      0.88065736 0.85       0.68333333 0.57708333]\n"
     ]
    }
   ],
   "source": [
    "class CRGAN:\n",
    "\n",
    "    def build_ranker(self):\n",
    "        r_u_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "        r_i_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "        Rank_Embedding_User = Embedding(input_dim=num_users, output_dim=int(ranker_layers[0] / 2),\n",
    "                                        name='rank_embedding_user',\n",
    "                                        init='random_normal', W_regularizer=l2(ranker_reg_layers[0]), input_length=1)\n",
    "        Rank_Embedding_Item = Embedding(input_dim=num_items, output_dim=int(ranker_layers[0] / 2),\n",
    "                                        name='rank_embedding_item',\n",
    "                                        init='random_normal', W_regularizer=l2(ranker_reg_layers[0]), input_length=1)\n",
    "\n",
    "        r_u_latent = Flatten()(Rank_Embedding_User(r_u_input))\n",
    "        r_i_latent = Flatten()(Rank_Embedding_Item(r_i_input))\n",
    "        vector = concatenate([r_u_latent, r_i_latent], axis=-1)\n",
    "        for idx in range(1, len(ranker_layers)):\n",
    "            layer = Dense(ranker_layers[idx], W_regularizer=l2(ranker_reg_layers[idx]), activation='relu',\n",
    "                          name='r_layer%d' % idx)\n",
    "            vector = layer(vector)\n",
    "        prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name='r_prediction')(vector)\n",
    "        ranker = Model(input=[r_u_input, r_i_input],\n",
    "                       output=prediction)\n",
    "        return ranker\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        d_u_input = Input((int((discriminator_layers[0] - 1) / 3),), dtype='float32', name='d_u_input')\n",
    "        d_i_input = Input((int((discriminator_layers[0] - 1) / 3),), dtype='float32', name='d_i_input')\n",
    "        d_j_input = Input((int((discriminator_layers[0] - 1) / 3),), dtype='float32', name='d_j_input')\n",
    "        d_r_input = Input((1,), dtype='float32', name='d_r_input')\n",
    "        d_input = concatenate([d_u_input, d_i_input, d_j_input, d_r_input], axis=-1)\n",
    "        vector = Dense(discriminator_layers[0], W_regularizer=l2(discriminator_reg_layers[0]), activation='relu',\n",
    "                       name='d_layer0')(d_input)\n",
    "        for idx in range(1, len(discriminator_layers)):\n",
    "            vector = Dense(discriminator_layers[idx], W_regularizer=l2(discriminator_reg_layers[idx]),\n",
    "                           activation='relu', name='d_layer%d' % idx)(vector)\n",
    "        prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name='d_prediction')(vector)\n",
    "        discriminator = Model(input=[d_u_input, d_i_input, d_j_input, d_r_input], output=prediction)\n",
    "        return discriminator\n",
    "\n",
    "    def __init__(self, d_lr=d_lr, r_lr=r_lr):\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.summary()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=Adam(d_lr),\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        u_input = Input(shape=(1,), dtype='float32', name='u_input')\n",
    "        i_input = Input(shape=(1,), dtype='float32', name='i_input')\n",
    "        j_input = Input(shape=(1,), dtype='float32', name='j_input')\n",
    "        Dis_Embedding_U = Embedding(input_dim=num_users, output_dim=int((discriminator_layers[0] - 1) / 3),\n",
    "                                    name='dis_embedding_u',\n",
    "                                    init='random_normal', W_regularizer=l2(discriminator_reg_layers[0]), input_length=1)\n",
    "        Dis_Embedding_I = Embedding(input_dim=num_items, output_dim=int((discriminator_layers[0] - 1) / 3),\n",
    "                                    name='dis_embedding_i',\n",
    "                                    init='random_normal', W_regularizer=l2(discriminator_reg_layers[0]), input_length=1)\n",
    "        Dis_Embedding_J = Embedding(input_dim=num_items, output_dim=int((discriminator_layers[0] - 1) / 3),\n",
    "                                    name='dis_embedding_j',\n",
    "                                    init='random_normal', W_regularizer=l2(discriminator_reg_layers[0]), input_length=1)\n",
    "\n",
    "        d_u_latent = Flatten()(Dis_Embedding_U(u_input))\n",
    "        d_i_latent = Flatten()(Dis_Embedding_I(i_input))\n",
    "        d_j_latent = Flatten()(Dis_Embedding_J(j_input))\n",
    "        self.ranker = self.build_ranker()\n",
    "        r_i = self.ranker([u_input, i_input])\n",
    "        r_j = self.ranker([u_input, j_input])\n",
    "        r = Subtract()([r_i, r_j])\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        y_pred = self.discriminator([d_u_latent, d_i_latent, d_j_latent, r])\n",
    "        self.combined = Model([u_input, i_input, j_input], y_pred)\n",
    "        self.combined.summary()\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=Adam(r_lr),\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        if pre_train is True:\n",
    "            self.ranker.load_weights(ranker_in_file)\n",
    "            # self.discriminator.load_weights(dis_in_file)\n",
    "        metrics = evaluate_model(self.ranker, testItems, testRatings, K)\n",
    "\n",
    "        print('init: ', metrics)\n",
    "        with open(log_file, 'w') as log:\n",
    "            print('-1', ' '.join('%.4f' % i for i in metrics), file=log)\n",
    "        best_metrics, best_epoch = metrics, -1\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            gen_r_i = self.ranker.predict([np.array(train_u_input), np.array(train_i_input)])\n",
    "            gen_r_j = self.ranker.predict([np.array(train_u_input), np.array(train_j_input)])\n",
    "            gen_r = gen_r_i - gen_r_j\n",
    "            # print(gen_r_j)\n",
    "\n",
    "            valid = np.ones_like(gen_r)\n",
    "            d_loss_real = self.discriminator.train_on_batch(\n",
    "                [train_u_latent, train_i_latent, train_j_latent, np.array(train_labels)],\n",
    "                valid)\n",
    "            fake = np.zeros_like(gen_r)\n",
    "            d_loss_gen = self.discriminator.train_on_batch(\n",
    "                [train_u_latent, train_i_latent, train_j_latent, gen_r], fake)\n",
    "            d_loss = (d_loss_real[0] + d_loss_gen[0]) / 2  # 0: loss, 1: acc\n",
    "            # print('epoch %d : d_loss = %.4f' % (epoch, d_loss))\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            g_loss = self.combined.train_on_batch([train_u_input, train_i_input, train_j_input], valid)\n",
    "            metrics = evaluate_model(self.ranker, testItems, testRatings, K)\n",
    "            with open(log_file, 'a') as log:\n",
    "                important_index = 1\n",
    "                if metrics[important_index] > best_metrics[important_index]:\n",
    "                    best_metrics, best_epoch = metrics, epoch\n",
    "                    print('epoch %d: ' % epoch, metrics, '[best]')\n",
    "                    print('%d' % epoch, ' '.join('%.4f' % i for i in metrics), g_loss[0], '[best]', file=log)\n",
    "                    if out is True:\n",
    "                        self.ranker.save_weights(ranker_out_file, overwrite=True)\n",
    "                        self.discriminator.save_weights(dis_out_file, overwrite=True)\n",
    "                else:\n",
    "                    print('epoch %d: ' % epoch, metrics)\n",
    "                    print('%d' % epoch, ' '.join('%.4f' % i for i in metrics), g_loss[0], file=log)\n",
    "        \n",
    "    \n",
    "cr_gan = CRGAN()\n",
    "cr_gan.train()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after training the model, we obtain all the users predicted ratings on all items, then call the aggregation function to obtain the groups predicted rating. Then, by Evaluation, we evaluate the difference between the predicted and real group ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [2, 2, 3, 2, 4, 5], 1: [3, 3, 3, 3, 4, 3], 2: [3, 3, 3, 3, 3, 3], 3: [4, 4, 3, 3, 2, 2], 4: [2, 4, 2, 3, 3, 3], 5: [2, 3, 3, 3, 4, 4], 6: [1, 3, 3, 3, 5, 3], 7: [3, 3, 3, 4, 3, 2], 8: [2, 2, 3, 3, 4, 4], 9: [2, 4, 2, 4, 3, 3], 10: [2, 2, 3, 3, 4, 4], 11: [3, 3, 3, 2, 4, 3], 12: [3, 3, 3, 3, 3, 3], 13: [4, 4, 3, 3, 2, 2], 14: [2, 4, 3, 3, 3, 3], 15: [2, 3, 3, 3, 4, 4], 16: [2, 3, 2, 3, 5, 3], 17: [2, 3, 3, 4, 3, 3], 18: [2, 2, 3, 3, 4, 4], 19: [2, 3, 3, 4, 4, 3]}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(AllItems)\n",
    "print(AllRatings)\n",
    "print(AllRatings[0][3])\n",
    "\n",
    "# Predicted ratings for all users on all items:\n",
    "#num_users = 20\n",
    "#num_items = 6\n",
    "PredictedRatings = dict()\n",
    "for user in range(num_users):\n",
    "    PredictedRatings[user] = []\n",
    "    # Assuming predict_user_ratings(Model, user, items) returns the ratings for the given user\n",
    "    ratings = predict_user_ratings(user, AllItems[0])  \n",
    "    # Flatten the nested list of ratings\n",
    "    flattened_ratings = [round(item, 2) for sublist in ratings for item in sublist]   \n",
    "    # Convert the ratings to float and append to the dictionary\n",
    "    PredictedRatings[user] = flattened_ratings\n",
    "\n",
    "#print(\"PredictedRatings:\", PredictedRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered_groups IDs list:\n",
      "[[1, 3, 4, 9, 10, 11, 12, 13, 15, 16], [2, 5, 6, 18, 19], [0, 14]]\n",
      "Average_Metrics:\n",
      " [1.   0.83 0.81 0.75 0.87 0.48 0.78 0.89 0.75 0.81 0.47]\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, fairness\n",
      "\n",
      " Program Executed in 0.014035300999999833\n"
     ]
    }
   ],
   "source": [
    "# Uploading the group IDs comming from SimGNN:\n",
    "\n",
    "file_path = \"./data/group_Mambers_IDs_%s.csv\"% dataset\n",
    "\n",
    "# Initialize an empty list to store the data from the CSV file\n",
    "Clustered_groups = []\n",
    "\n",
    "# Read the CSV file and populate the 2D list\n",
    "with open(file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        # Convert each row to integers and append it to the 2D list\n",
    "        row_int = [int(cell)-1 for cell in row]\n",
    "        Clustered_groups.append(row_int)\n",
    "\n",
    "print(\"Clustered_groups IDs list:\")\n",
    "print(Clustered_groups)\n",
    "\n",
    "\n",
    "Groups_ActualRatings = Aggregation(Clustered_groups, AllItems, AllRatings)   \n",
    "Groups_PredictedRatings = Aggregation(Clustered_groups, AllItems, PredictedRatings)  *5\n",
    "start = timeit.default_timer()\n",
    "#print(\"AllItems: \\n\", AllItems)\n",
    "#print(\"Groups_ActualRatings: \\n\", Groups_ActualRatings)\n",
    "items = [i for i in range(1, num_items)]\n",
    "METRICS = np.zeros([3,11])\n",
    "for groupID in range(0,3):\n",
    "    METRICS[groupID] = eval_groups(groupID, Clustered_groups, Groups_ActualRatings, Groups_PredictedRatings, items, AllRatings)\n",
    "Average_Metrics = np.round(np.mean(METRICS, axis=0),2)\n",
    "print(\"Average_Metrics:\\n\", Average_Metrics )#print(\"Groups_PredictedRatings: \\n\", Groups_PredictedRatings)\n",
    "print(\"hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, fairness\")\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "print(\"\\n Program Executed in \"+str(execution_time)) # It returns time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average_Metrics:\n",
      " [1.   0.83 0.88 0.88 0.95 0.54 0.78 0.89 0.75 0.81 0.54] 0.010978895000000932\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.84 0.85 0.9  0.49 0.72 0.89 0.67 0.76 0.4 ] 0.010344864000000342\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.83 0.81 0.88 0.51 0.67 0.78 0.64 0.7  0.52] 0.011662291999998686\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.78 0.74 0.74 0.78 0.49 0.78 0.78 0.78 0.78 0.75] 0.01079312500000107\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.67 0.72 0.78 0.83 0.54 0.72 0.7  0.78 0.73 0.52] 0.011608089999999294\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.67 0.75 0.78 0.86 0.52 0.61 0.67 0.58 0.62 0.49] 0.01117822600000018\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.77 0.72 0.81 0.43 0.61 0.78 0.59 0.66 0.52] 0.011611043999998572\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.78 0.77 0.78 0.82 0.48 0.72 0.78 0.7  0.73 0.79] 0.012072769999999622\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.83 0.81 0.88 0.51 0.67 0.78 0.64 0.7  0.36] 0.011475754000001004\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.92 0.93 0.92 0.96 0.5  0.67 0.89 0.67 0.75 0.27] 0.010172263999999487\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.58 0.52 0.47 0.63 0.35 0.5  0.56 0.5  0.52 0.47] 0.01185451999999998\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.82 0.8  0.87 0.48 0.61 0.78 0.59 0.66 0.52] 0.011565349999999697\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   1.   1.   1.   1.   0.51 0.78 1.   0.75 0.84 0.37] 0.010995279999999497\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.87 0.89 0.94 0.51 0.72 0.89 0.67 0.76 0.6 ] 0.012329377000000363\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.89 0.92 0.93 0.96 0.55 0.78 0.89 0.76 0.81 0.29] 0.010922380000000231\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.78 0.77 0.82 0.84 0.51 0.78 0.78 0.78 0.78 0.49] 0.010307517000001098\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.87 0.89 0.94 0.51 0.72 0.89 0.67 0.76 0.44] 0.012864975999999473\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.81 0.86 0.88 0.94 0.56 0.72 0.78 0.72 0.75 0.59] 0.010835473999998513\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.89 0.84 0.89 0.88 0.45 0.72 0.89 0.67 0.76 0.35] 0.011963439999998826\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "Average_Metrics:\n",
      " [1.   0.89 0.92 0.96 0.97 0.53 0.72 0.89 0.72 0.78 0.36] 0.012872759999998706\n",
      "hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\n",
      "****** Final_Metrics: [1.   0.81 0.82 0.83 0.88 0.5  0.7  0.81 0.68 0.73 0.48]\n"
     ]
    }
   ],
   "source": [
    "# Making random groups with random size:\n",
    "# Repeating the experiment 20 times and get the average of metrics.\n",
    "\n",
    "num_repeat = 20\n",
    "num_Mertics = 11\n",
    "TotallMetrics = []\n",
    "\n",
    "for i in range(num_repeat):    \n",
    "    Random_groups = create_random_groups(Clustered_groups, num_users)\n",
    "    # print(Random_groups)\n",
    "\n",
    "    Groups_ActualRatings = Aggregation(Random_groups, AllItems, AllRatings)   \n",
    "    Groups_PredictedRatings = Aggregation(Random_groups, AllItems, PredictedRatings) * 5\n",
    "\n",
    "    # print(\"AllItems: \\n\", AllItems)\n",
    "    # print(\"Groups_ActualRatings: \\n\", Groups_ActualRatings)\n",
    "    # print(\"Groups_PredictedRatings: \\n\", Groups_PredictedRatings)\n",
    "    start = timeit.default_timer()\n",
    "    items = [i for i in range(1, num_items)]\n",
    "    METRICS = np.zeros([3, num_Mertics])\n",
    "    for groupID in range(3):\n",
    "        METRICS[groupID] = eval_groups(groupID, Random_groups, Groups_ActualRatings, Groups_PredictedRatings, items, AllRatings)\n",
    "    Average_Metrics = np.round(np.mean(METRICS, axis=0), 2)\n",
    "    stop = timeit.default_timer()\n",
    "    execution_time = stop - start\n",
    "    print(\"Average_Metrics:\\n\", Average_Metrics, execution_time)\n",
    "    print(\"hr, p, ndcg_bin, auc, map, mrr, accuracy, precision, recall, f1, execution_time\")\n",
    "    # print(\"\\n Program Executed in \"+str(execution_time)) # It returns time in seconds\n",
    "    TotallMetrics.append(list(Average_Metrics))\n",
    "\n",
    "Final_Metrics = np.round(np.mean(TotallMetrics, axis=0), 2)\n",
    "print(\"****** Final_Metrics:\", Final_Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
